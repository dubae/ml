{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UUVzbL5_K1TT"
   },
   "source": [
    "# Machine Learning Assignment \\#2 - Image Captioning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_xMX29o44zW6"
   },
   "source": [
    "## Unzip training and validation images\n",
    "- First, you must download the data files from link in assignment description, then you must move these .zip files to Colab environment in \"/content/\" path.\n",
    "- If you use individual environment, you can apply all variables to your environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "5n7CMSTPx9wR"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "train_zip_file = zipfile.ZipFile('content/train_images.zip')\n",
    "train_zip_file.extractall('content/')\n",
    "\n",
    "valid_zip_file = zipfile.ZipFile('content/valid_images.zip')\n",
    "valid_zip_file.extractall('content/')\n",
    "\n",
    "## local 환경에 맞춰서 경로 수정함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qHKB5_Nmbj1f"
   },
   "source": [
    "## Set seed value of several main computation packages (do not touch this settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "eqwqfc3IbcFX"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "random_seed = 1\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.manual_seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aOS0jus_GFzZ"
   },
   "source": [
    "## Create Vocabulary Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m5EWyibwJGAF"
   },
   "source": [
    "## Dataloader for dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "M2Ujt30s-505"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from PIL import Image\n",
    "\n",
    "import torch.utils.data as data\n",
    "\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('punkt')\n",
    "class ImageCaptioningDataset(data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root,                         # root path of resized data\n",
    "        captions_path,                # path of original text data\n",
    "        vocab, transform=None,\n",
    "        is_train=True,                # check whether the dataset is for training or not\n",
    "        gen_train_captions_path=None, # path for generated caption data\n",
    "        n_gen_captions=None           # the number of generated captions that are used for training. You can change this value. If this variable is 0, only original caption data will be used.\n",
    "        ):\n",
    "        self.root = root\n",
    "        self.captions = []\n",
    "\n",
    "        with open(captions_path, 'rb') as f:\n",
    "          captions = pickle.load(f)\n",
    "\n",
    "        for fname, caps in captions:\n",
    "            if is_train:\n",
    "              for cap in caps:\n",
    "                    self.captions.append((fname, cap.strip()))\n",
    "\n",
    "            else:\n",
    "                for cap in caps:\n",
    "                    self.captions.append((fname, cap.strip()))\n",
    "\n",
    "        if is_train:\n",
    "          ################################################################################################################################\n",
    "          # FILL BLANK #1: load generated captions and append (image, generated_caption) pairs with the value of n_gen_captions.\n",
    "          with open(gen_train_captions_path, 'rb') as f:\n",
    "              gen_captions = pickle.load(f)\n",
    "          for fname, caps in gen_captions:\n",
    "              for i, cap in enumerate(caps):\n",
    "                  if i < n_gen_captions:\n",
    "                      self.captions.append((fname, cap.strip()))\n",
    "          ################################################################################################################################\n",
    "\n",
    "        self.vocab = vocab\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        vocab = self.vocab\n",
    "        path = self.captions[index][0]\n",
    "        caption = self.captions[index][1]\n",
    "\n",
    "        image = Image.open(os.path.join(self.root, path)).convert('RGB')\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Tokenize caption string\n",
    "        tokens = nltk.tokenize.word_tokenize(str(caption).lower())\n",
    "        caption = []\n",
    "\n",
    "        ################################################################################################################################\n",
    "        # FILL BLANK #2: Caption tokens should start with the start token and end with the end token, and should be list type.\n",
    "        caption.append(vocab('<start>'))\n",
    "        caption.extend([vocab(token) for token in tokens])\n",
    "        caption.append(vocab('<end>'))\n",
    "        ################################################################################################################################\n",
    "\n",
    "        target = torch.Tensor(caption)\n",
    "        return image, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "mWOnyplGFM-J"
   },
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    \"\"\"\n",
    "    [input]\n",
    "    * data: list of tuple (image, caption).\n",
    "        * image: torch tensor of shape (3, 256, 256).\n",
    "        * caption: torch tensor of shape (?); variable length.\n",
    "    [output]\n",
    "    * images: torch tensor of shape (batch_size, 3, 256, 256).\n",
    "    * targets: torch tensor of shape (batch_size, padded_length).\n",
    "    * lengths: list; valid length for each padded caption.\n",
    "    \"\"\"\n",
    "    data.sort(key=lambda x: len(x[1]), reverse=True)\n",
    "    images, captions = zip(*data)\n",
    "\n",
    "    images = torch.stack(images, 0)\n",
    "\n",
    "    lengths = [len(caption) for caption in captions]\n",
    "    targets = torch.zeros(len(captions), max(lengths)).long()\n",
    "    for i, cap in enumerate(captions):\n",
    "        end = lengths[i]\n",
    "        targets[i, :end] = cap[:end]\n",
    "    return images, targets, lengths\n",
    "\n",
    "def collate_fn_test(data):\n",
    "    images, captions = zip(*data)\n",
    "\n",
    "    images = torch.stack(images, 0)\n",
    "\n",
    "    lengths = [len(caption) for caption in captions]\n",
    "    targets = torch.zeros(len(captions), max(lengths)).long()\n",
    "    for i, cap in enumerate(captions):\n",
    "        end = lengths[i]\n",
    "        targets[i, :end] = cap[:end]\n",
    "    return images, targets, lengths\n",
    "\n",
    "def get_loader(root, captions_path, vocab, transform, batch_size, shuffle, num_workers, testing, is_train,\n",
    "               gen_train_captions_path=None, n_gen_captions=None):\n",
    "    image_captioning_dataset = ImageCaptioningDataset(root=root, captions_path=captions_path, vocab=vocab, transform=transform, is_train=is_train, gen_train_captions_path=gen_train_captions_path, n_gen_captions=n_gen_captions)\n",
    "    # This will return (images, captions, lengths) for each iteration by collate_fn in Dataloader.\n",
    "    # images: a tensor of shape (batch_size, 3, 224, 224).\n",
    "    # captions: a tensor of shape (batch_size, padded_length).\n",
    "    # lengths: a list indicating valid length for each caption. length is (batch_size).\n",
    "    if not testing:\n",
    "        data_loader = torch.utils.data.DataLoader(dataset=image_captioning_dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers, collate_fn=collate_fn)\n",
    "    else:\n",
    "        data_loader = torch.utils.data.DataLoader(dataset=image_captioning_dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers, collate_fn=collate_fn_test)\n",
    "    return data_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YG3cEqenJPeP"
   },
   "source": [
    "# Train Image Captioning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QlR444cq92zl"
   },
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "y-tJTLdL92eD"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "\n",
    "\n",
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        ################################################################################################################################\n",
    "        '''\n",
    "        FILL BLANK #3:\n",
    "          - load pretrained resnet152 by torchvision.models\n",
    "          - drop the last layer.\n",
    "          - modulate pretrained resnet\n",
    "          - attach an embedding layer to integrate text embedding (momentum value: 0.01)\n",
    "        '''\n",
    "        resnet      = models.resnet152(pretrained=True)\n",
    "        modules     = list(resnet.children())[:-1]  # drop the last layer (fully connected)\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        self.linear = nn.Linear(resnet.fc.in_features, embed_size)\n",
    "        self.bn     = nn.BatchNorm1d(embed_size, momentum=0.01)\n",
    "        ################################################################################################################################\n",
    "\n",
    "\n",
    "    def forward(self, images):\n",
    "        with torch.no_grad():\n",
    "            features = self.resnet(images)\n",
    "        features = features.reshape(features.size(0), -1)\n",
    "        features = self.bn(self.linear(features))\n",
    "        return features\n",
    "\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers, max_seq_length=20):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "\n",
    "        ################################################################################################################################\n",
    "        #FILL BLANK #4: define text embedding layer and lstm layer\n",
    "        self.embed  = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm   = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        ################################################################################################################################\n",
    "\n",
    "        self.max_seg_length = max_seq_length\n",
    "\n",
    "\n",
    "    def forward(self, features, captions, lengths):\n",
    "        embeddings = self.embed(captions)\n",
    "        embeddings = torch.cat((features.unsqueeze(1), embeddings), 1)\n",
    "        packed = pack_padded_sequence(embeddings, lengths, batch_first=True)\n",
    "        hiddens, _ = self.lstm(packed)\n",
    "        outputs = self.linear(hiddens[0])\n",
    "        return outputs\n",
    "\n",
    "    def sample(self, features, states=None):\n",
    "        sampled_indexes = []\n",
    "        inputs = features.unsqueeze(1)\n",
    "        for i in range(self.max_seg_length):\n",
    "            hiddens, states = self.lstm(inputs, states)   # hiddens: (batch_size, 1, hidden_size)\n",
    "            outputs = self.linear(hiddens.squeeze(1))     # outputs: (batch_size, vocab_size)\n",
    "            _, predicted = outputs.max(1)                 # predicted: (batch_size)\n",
    "            sampled_indexes.append(predicted)\n",
    "            inputs = self.embed(predicted)                # inputs: (batch_size, embed_size)\n",
    "            inputs = inputs.unsqueeze(1)                  # inputs: (batch_size, 1, embed_size)\n",
    "        sampled_indexes = torch.stack(sampled_indexes, 1) # sampled_indexes: (batch_size, max_seq_length)\n",
    "        return sampled_indexes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2baXOLKgpDV_"
   },
   "source": [
    "## Vocabulay Class Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "sVYbyf0TpCgV"
   },
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    \"\"\"Simple vocabulary wrapper.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.idx = 0\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if not word in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "\n",
    "    def __call__(self, word):\n",
    "        if not word in self.word2idx:\n",
    "            return self.word2idx['']\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RrR5xvcS-NcG"
   },
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ca-uLHPG-GVy"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torchvision import transforms\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "crop_size = 224\n",
    "\n",
    "# 원래 경로\n",
    "# train_image_dir = \"./train_images\" # training image path (train)\n",
    "# valid_image_dir = \"./valid_images\"   # validation image path (valid)\n",
    "\n",
    "# train_captions_path = \"./train_captions.pkl\"\n",
    "# valid_captions_path = \"./valid_captions.pkl\"\n",
    "# vocab_path = \"./vocab.pkl\" # pre-processsed vocab file path\n",
    "\n",
    "# 로컬 환경에 맞도록 수정한 경로\n",
    "train_image_dir = \"content/train_images\" # training image path (train)\n",
    "valid_image_dir = \"content/valid_images\"   # validation image path (valid)\n",
    "\n",
    "train_captions_path = \"content/train_captions.pkl\"\n",
    "valid_captions_path = \"content/valid_captions.pkl\"\n",
    "vocab_path=\"content/vocab.pkl\"\n",
    "\n",
    "# make directory to save model\n",
    "model_path = \"./models/\"   # save model path\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "\n",
    "# Load vocabulary flie\n",
    "with open(vocab_path, 'rb') as f:\n",
    "    vocab = pickle.load(f)\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomCrop(crop_size),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(crop_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
    "\n",
    "batch_size = 128\n",
    "num_workers = 0 ## <- 2 / 로컬 환경에 맞도록 2->0으로 수정.\n",
    "\n",
    "n_gen_train_captions = 0     # Decide the number of generated captions for each image (integer)\n",
    "\n",
    "# 원래 경로\n",
    "# VLM_gen_train_captions_path = \"./generated_captions.pkl\" # path of generated captions\n",
    "\n",
    "# 로컬 환경에 맞도록 경로 수정\n",
    "VLM_gen_train_captions_path = \"content/generated_captions.pkl\" # path of generated captions\n",
    "\n",
    "\n",
    "train_data_loader = get_loader(train_image_dir, train_captions_path, vocab, train_transform, batch_size, shuffle=True, num_workers=num_workers, testing=False, is_train=True,\n",
    "                               gen_train_captions_path=VLM_gen_train_captions_path, n_gen_captions=n_gen_train_captions)\n",
    "valid_data_loader = get_loader(valid_image_dir, valid_captions_path, vocab, val_transform, batch_size, shuffle=False, num_workers=num_workers, testing=False, is_train=False)\n",
    "\n",
    "\n",
    "embed_size = 256\n",
    "hidden_size = 512\n",
    "num_layers = 1\n",
    "\n",
    "encoder = EncoderCNN(embed_size).to(device)\n",
    "decoder = DecoderRNN(embed_size, hidden_size, len(vocab), num_layers).to(device)\n",
    "\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "################################################################################################################################\n",
    "# FILL BLANK #5 : Use cross entropy loss for image captioning model\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "################################################################################################################################\n",
    "\n",
    "# get trainable parameters (freeze the pretrained ResNet backbone).\n",
    "params = list(decoder.parameters()) + list(encoder.linear.parameters()) + list(encoder.bn.parameters())\n",
    "optimizer = torch.optim.Adam(params, lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "gRbXZbjx-M6U"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Training ]\n",
      "Epoch [1/10], Step [0/235], Average Loss: 0.0636, Perplexity: 3447.7605, Elapsed time: 0.8638s\n",
      "Epoch [1/10], Step [20/235], Average Loss: 0.0487, Perplexity: 176.3974, Elapsed time: 18.2856s\n",
      "Epoch [1/10], Step [40/235], Average Loss: 0.0427, Perplexity: 71.9925, Elapsed time: 35.7968s\n",
      "Epoch [1/10], Step [60/235], Average Loss: 0.0393, Perplexity: 60.7641, Elapsed time: 53.5451s\n",
      "Epoch [1/10], Step [80/235], Average Loss: 0.0370, Perplexity: 42.1145, Elapsed time: 71.1505s\n",
      "Epoch [1/10], Step [100/235], Average Loss: 0.0354, Perplexity: 35.9566, Elapsed time: 89.4146s\n",
      "Epoch [1/10], Step [120/235], Average Loss: 0.0342, Perplexity: 36.5266, Elapsed time: 107.1721s\n",
      "Epoch [1/10], Step [140/235], Average Loss: 0.0333, Perplexity: 33.5420, Elapsed time: 124.8035s\n",
      "Epoch [1/10], Step [160/235], Average Loss: 0.0325, Perplexity: 31.2103, Elapsed time: 142.4682s\n",
      "Epoch [1/10], Step [180/235], Average Loss: 0.0318, Perplexity: 27.9156, Elapsed time: 160.1455s\n",
      "Epoch [1/10], Step [200/235], Average Loss: 0.0312, Perplexity: 26.8179, Elapsed time: 177.8883s\n",
      "Epoch [1/10], Step [220/235], Average Loss: 0.0307, Perplexity: 27.2770, Elapsed time: 195.9053s\n",
      "Model saved: ./models/decoder-1.ckpt\n",
      "Model saved: ./models/encoder-1.ckpt\n",
      "[ Validation ]\n",
      "Epoch [1/10], Step [0/40], Average Loss: 0.0259, Perplexity: 27.6337, Elapsed time: 209.6781s\n",
      "Epoch [1/10], Step [20/40], Average Loss: 0.0255, Perplexity: 31.4985, Elapsed time: 229.3268s\n",
      "Best Model saved at 1: ./models/decoder-best.ckpt\n",
      "Best Model saved at 1: ./models/encoder-best.ckpt\n",
      "[ Training ]\n",
      "Epoch [2/10], Step [0/235], Average Loss: 0.0242, Perplexity: 22.1848, Elapsed time: 248.7556s\n",
      "Epoch [2/10], Step [20/235], Average Loss: 0.0244, Perplexity: 20.1671, Elapsed time: 266.6262s\n",
      "Epoch [2/10], Step [40/235], Average Loss: 0.0245, Perplexity: 24.1833, Elapsed time: 284.4058s\n",
      "Epoch [2/10], Step [60/235], Average Loss: 0.0243, Perplexity: 21.3414, Elapsed time: 302.1276s\n",
      "Epoch [2/10], Step [80/235], Average Loss: 0.0242, Perplexity: 17.7066, Elapsed time: 319.9122s\n",
      "Epoch [2/10], Step [100/235], Average Loss: 0.0241, Perplexity: 19.3191, Elapsed time: 337.6391s\n",
      "Epoch [2/10], Step [120/235], Average Loss: 0.0240, Perplexity: 20.6613, Elapsed time: 355.3472s\n",
      "Epoch [2/10], Step [140/235], Average Loss: 0.0239, Perplexity: 16.5868, Elapsed time: 373.2420s\n",
      "Epoch [2/10], Step [160/235], Average Loss: 0.0238, Perplexity: 17.1256, Elapsed time: 390.9770s\n",
      "Epoch [2/10], Step [180/235], Average Loss: 0.0237, Perplexity: 19.9823, Elapsed time: 408.8324s\n",
      "Epoch [2/10], Step [200/235], Average Loss: 0.0236, Perplexity: 17.4177, Elapsed time: 426.5849s\n",
      "Epoch [2/10], Step [220/235], Average Loss: 0.0235, Perplexity: 17.5482, Elapsed time: 444.3046s\n",
      "Model saved: ./models/decoder-2.ckpt\n",
      "Model saved: ./models/encoder-2.ckpt\n",
      "[ Validation ]\n",
      "Epoch [2/10], Step [0/40], Average Loss: 0.0236, Perplexity: 20.3989, Elapsed time: 457.5688s\n",
      "Epoch [2/10], Step [20/40], Average Loss: 0.0232, Perplexity: 22.7946, Elapsed time: 476.9939s\n",
      "Best Model saved at 2: ./models/decoder-best.ckpt\n",
      "Best Model saved at 2: ./models/encoder-best.ckpt\n",
      "[ Training ]\n",
      "Epoch [3/10], Step [0/235], Average Loss: 0.0215, Perplexity: 15.5984, Elapsed time: 496.1627s\n",
      "Epoch [3/10], Step [20/235], Average Loss: 0.0218, Perplexity: 14.0881, Elapsed time: 514.3029s\n",
      "Epoch [3/10], Step [40/235], Average Loss: 0.0217, Perplexity: 16.1290, Elapsed time: 532.2088s\n",
      "Epoch [3/10], Step [60/235], Average Loss: 0.0216, Perplexity: 15.0749, Elapsed time: 549.9167s\n",
      "Epoch [3/10], Step [80/235], Average Loss: 0.0216, Perplexity: 14.1017, Elapsed time: 567.7464s\n",
      "Epoch [3/10], Step [100/235], Average Loss: 0.0215, Perplexity: 17.5817, Elapsed time: 585.9409s\n",
      "Epoch [3/10], Step [120/235], Average Loss: 0.0215, Perplexity: 16.1704, Elapsed time: 603.7210s\n",
      "Epoch [3/10], Step [140/235], Average Loss: 0.0215, Perplexity: 15.5703, Elapsed time: 621.4500s\n",
      "Epoch [3/10], Step [160/235], Average Loss: 0.0214, Perplexity: 14.2761, Elapsed time: 639.4640s\n",
      "Epoch [3/10], Step [180/235], Average Loss: 0.0214, Perplexity: 14.0531, Elapsed time: 657.1886s\n",
      "Epoch [3/10], Step [200/235], Average Loss: 0.0213, Perplexity: 15.1008, Elapsed time: 674.9503s\n",
      "Epoch [3/10], Step [220/235], Average Loss: 0.0212, Perplexity: 13.7883, Elapsed time: 692.7522s\n",
      "Model saved: ./models/decoder-3.ckpt\n",
      "Model saved: ./models/encoder-3.ckpt\n",
      "[ Validation ]\n",
      "Epoch [3/10], Step [0/40], Average Loss: 0.0224, Perplexity: 17.5777, Elapsed time: 706.3304s\n",
      "Epoch [3/10], Step [20/40], Average Loss: 0.0221, Perplexity: 19.4139, Elapsed time: 725.7518s\n",
      "Best Model saved at 3: ./models/decoder-best.ckpt\n",
      "Best Model saved at 3: ./models/encoder-best.ckpt\n",
      "[ Training ]\n",
      "Epoch [4/10], Step [0/235], Average Loss: 0.0200, Perplexity: 12.9443, Elapsed time: 744.8452s\n",
      "Epoch [4/10], Step [20/235], Average Loss: 0.0199, Perplexity: 11.7920, Elapsed time: 762.6089s\n",
      "Epoch [4/10], Step [40/235], Average Loss: 0.0199, Perplexity: 12.8951, Elapsed time: 780.6062s\n",
      "Epoch [4/10], Step [60/235], Average Loss: 0.0199, Perplexity: 12.2622, Elapsed time: 799.1716s\n",
      "Epoch [4/10], Step [80/235], Average Loss: 0.0199, Perplexity: 11.7115, Elapsed time: 817.2299s\n",
      "Epoch [4/10], Step [100/235], Average Loss: 0.0199, Perplexity: 13.5091, Elapsed time: 835.7624s\n",
      "Epoch [4/10], Step [120/235], Average Loss: 0.0199, Perplexity: 12.5595, Elapsed time: 853.5448s\n",
      "Epoch [4/10], Step [140/235], Average Loss: 0.0199, Perplexity: 11.7089, Elapsed time: 871.2974s\n",
      "Epoch [4/10], Step [160/235], Average Loss: 0.0199, Perplexity: 10.5577, Elapsed time: 889.3164s\n",
      "Epoch [4/10], Step [180/235], Average Loss: 0.0198, Perplexity: 13.1823, Elapsed time: 907.2075s\n",
      "Epoch [4/10], Step [200/235], Average Loss: 0.0198, Perplexity: 12.3367, Elapsed time: 925.2658s\n",
      "Epoch [4/10], Step [220/235], Average Loss: 0.0198, Perplexity: 12.6370, Elapsed time: 943.2100s\n",
      "Model saved: ./models/decoder-4.ckpt\n",
      "Model saved: ./models/encoder-4.ckpt\n",
      "[ Validation ]\n",
      "Epoch [4/10], Step [0/40], Average Loss: 0.0219, Perplexity: 16.4092, Elapsed time: 956.5225s\n",
      "Epoch [4/10], Step [20/40], Average Loss: 0.0216, Perplexity: 18.3794, Elapsed time: 975.9316s\n",
      "Best Model saved at 4: ./models/decoder-best.ckpt\n",
      "Best Model saved at 4: ./models/encoder-best.ckpt\n",
      "[ Training ]\n",
      "Epoch [5/10], Step [0/235], Average Loss: 0.0184, Perplexity: 10.5771, Elapsed time: 994.9530s\n",
      "Epoch [5/10], Step [20/235], Average Loss: 0.0185, Perplexity: 9.8272, Elapsed time: 1012.6264s\n",
      "Epoch [5/10], Step [40/235], Average Loss: 0.0186, Perplexity: 12.6686, Elapsed time: 1030.3511s\n",
      "Epoch [5/10], Step [60/235], Average Loss: 0.0187, Perplexity: 10.6490, Elapsed time: 1048.0357s\n",
      "Epoch [5/10], Step [80/235], Average Loss: 0.0187, Perplexity: 10.2531, Elapsed time: 1065.8153s\n",
      "Epoch [5/10], Step [100/235], Average Loss: 0.0187, Perplexity: 10.6644, Elapsed time: 1083.5096s\n",
      "Epoch [5/10], Step [120/235], Average Loss: 0.0186, Perplexity: 10.9433, Elapsed time: 1101.2247s\n",
      "Epoch [5/10], Step [140/235], Average Loss: 0.0187, Perplexity: 11.9120, Elapsed time: 1118.8997s\n",
      "Epoch [5/10], Step [160/235], Average Loss: 0.0187, Perplexity: 11.3498, Elapsed time: 1136.5979s\n",
      "Epoch [5/10], Step [180/235], Average Loss: 0.0187, Perplexity: 9.9224, Elapsed time: 1154.3828s\n",
      "Epoch [5/10], Step [200/235], Average Loss: 0.0187, Perplexity: 10.5307, Elapsed time: 1172.0545s\n",
      "Epoch [5/10], Step [220/235], Average Loss: 0.0186, Perplexity: 10.3006, Elapsed time: 1189.7376s\n",
      "Model saved: ./models/decoder-5.ckpt\n",
      "Model saved: ./models/encoder-5.ckpt\n",
      "[ Validation ]\n",
      "Epoch [5/10], Step [0/40], Average Loss: 0.0215, Perplexity: 15.5757, Elapsed time: 1202.9701s\n",
      "Epoch [5/10], Step [20/40], Average Loss: 0.0211, Perplexity: 17.0952, Elapsed time: 1222.3918s\n",
      "Best Model saved at 5: ./models/decoder-best.ckpt\n",
      "Best Model saved at 5: ./models/encoder-best.ckpt\n",
      "[ Training ]\n",
      "Epoch [6/10], Step [0/235], Average Loss: 0.0179, Perplexity: 9.9460, Elapsed time: 1241.3825s\n",
      "Epoch [6/10], Step [20/235], Average Loss: 0.0176, Perplexity: 9.4280, Elapsed time: 1259.0459s\n",
      "Epoch [6/10], Step [40/235], Average Loss: 0.0176, Perplexity: 9.3177, Elapsed time: 1276.6838s\n",
      "Epoch [6/10], Step [60/235], Average Loss: 0.0177, Perplexity: 10.8033, Elapsed time: 1294.9956s\n",
      "Epoch [6/10], Step [80/235], Average Loss: 0.0176, Perplexity: 9.1423, Elapsed time: 1312.7908s\n",
      "Epoch [6/10], Step [100/235], Average Loss: 0.0177, Perplexity: 9.6075, Elapsed time: 1330.8448s\n",
      "Epoch [6/10], Step [120/235], Average Loss: 0.0177, Perplexity: 9.7026, Elapsed time: 1348.6698s\n",
      "Epoch [6/10], Step [140/235], Average Loss: 0.0177, Perplexity: 9.1845, Elapsed time: 1366.4807s\n",
      "Epoch [6/10], Step [160/235], Average Loss: 0.0176, Perplexity: 9.0805, Elapsed time: 1384.1028s\n",
      "Epoch [6/10], Step [180/235], Average Loss: 0.0176, Perplexity: 10.3850, Elapsed time: 1401.7108s\n",
      "Epoch [6/10], Step [200/235], Average Loss: 0.0177, Perplexity: 10.1366, Elapsed time: 1419.3110s\n",
      "Epoch [6/10], Step [220/235], Average Loss: 0.0176, Perplexity: 9.0122, Elapsed time: 1437.0188s\n",
      "Model saved: ./models/decoder-6.ckpt\n",
      "Model saved: ./models/encoder-6.ckpt\n",
      "[ Validation ]\n",
      "Epoch [6/10], Step [0/40], Average Loss: 0.0213, Perplexity: 15.2332, Elapsed time: 1450.3069s\n",
      "Epoch [6/10], Step [20/40], Average Loss: 0.0209, Perplexity: 16.8025, Elapsed time: 1469.6191s\n",
      "Best Model saved at 6: ./models/decoder-best.ckpt\n",
      "Best Model saved at 6: ./models/encoder-best.ckpt\n",
      "[ Training ]\n",
      "Epoch [7/10], Step [0/235], Average Loss: 0.0171, Perplexity: 8.9713, Elapsed time: 1488.6322s\n",
      "Epoch [7/10], Step [20/235], Average Loss: 0.0166, Perplexity: 7.4979, Elapsed time: 1506.2593s\n",
      "Epoch [7/10], Step [40/235], Average Loss: 0.0167, Perplexity: 8.4664, Elapsed time: 1523.8363s\n",
      "Epoch [7/10], Step [60/235], Average Loss: 0.0167, Perplexity: 9.2726, Elapsed time: 1541.4428s\n",
      "Epoch [7/10], Step [80/235], Average Loss: 0.0167, Perplexity: 8.8297, Elapsed time: 1559.0217s\n",
      "Epoch [7/10], Step [100/235], Average Loss: 0.0167, Perplexity: 8.9432, Elapsed time: 1576.6197s\n",
      "Epoch [7/10], Step [120/235], Average Loss: 0.0168, Perplexity: 9.1788, Elapsed time: 1594.1661s\n",
      "Epoch [7/10], Step [140/235], Average Loss: 0.0168, Perplexity: 8.5658, Elapsed time: 1611.7756s\n",
      "Epoch [7/10], Step [160/235], Average Loss: 0.0168, Perplexity: 8.2424, Elapsed time: 1629.6311s\n",
      "Epoch [7/10], Step [180/235], Average Loss: 0.0168, Perplexity: 8.4836, Elapsed time: 1647.8055s\n",
      "Epoch [7/10], Step [200/235], Average Loss: 0.0168, Perplexity: 8.9152, Elapsed time: 1665.6578s\n",
      "Epoch [7/10], Step [220/235], Average Loss: 0.0168, Perplexity: 9.2979, Elapsed time: 1683.2859s\n",
      "Model saved: ./models/decoder-7.ckpt\n",
      "Model saved: ./models/encoder-7.ckpt\n",
      "[ Validation ]\n",
      "Epoch [7/10], Step [0/40], Average Loss: 0.0210, Perplexity: 14.6950, Elapsed time: 1696.5148s\n",
      "Epoch [7/10], Step [20/40], Average Loss: 0.0208, Perplexity: 16.5885, Elapsed time: 1715.9410s\n",
      "Best Model saved at 7: ./models/decoder-best.ckpt\n",
      "Best Model saved at 7: ./models/encoder-best.ckpt\n",
      "[ Training ]\n",
      "Epoch [8/10], Step [0/235], Average Loss: 0.0162, Perplexity: 7.9346, Elapsed time: 1735.2089s\n",
      "Epoch [8/10], Step [20/235], Average Loss: 0.0158, Perplexity: 7.2573, Elapsed time: 1752.7935s\n",
      "Epoch [8/10], Step [40/235], Average Loss: 0.0158, Perplexity: 7.9123, Elapsed time: 1770.4103s\n",
      "Epoch [8/10], Step [60/235], Average Loss: 0.0158, Perplexity: 8.1865, Elapsed time: 1788.3618s\n",
      "Epoch [8/10], Step [80/235], Average Loss: 0.0159, Perplexity: 7.4509, Elapsed time: 1806.5111s\n",
      "Epoch [8/10], Step [100/235], Average Loss: 0.0159, Perplexity: 8.1541, Elapsed time: 1824.1116s\n",
      "Epoch [8/10], Step [120/235], Average Loss: 0.0160, Perplexity: 8.3239, Elapsed time: 1841.7104s\n",
      "Epoch [8/10], Step [140/235], Average Loss: 0.0160, Perplexity: 7.2399, Elapsed time: 1859.3340s\n",
      "Epoch [8/10], Step [160/235], Average Loss: 0.0160, Perplexity: 8.0662, Elapsed time: 1877.0752s\n",
      "Epoch [8/10], Step [180/235], Average Loss: 0.0160, Perplexity: 7.6851, Elapsed time: 1894.9032s\n",
      "Epoch [8/10], Step [200/235], Average Loss: 0.0160, Perplexity: 7.7428, Elapsed time: 1912.5698s\n",
      "Epoch [8/10], Step [220/235], Average Loss: 0.0160, Perplexity: 8.2238, Elapsed time: 1930.2441s\n",
      "Model saved: ./models/decoder-8.ckpt\n",
      "Model saved: ./models/encoder-8.ckpt\n",
      "[ Validation ]\n",
      "Epoch [8/10], Step [0/40], Average Loss: 0.0210, Perplexity: 14.6192, Elapsed time: 1943.4249s\n",
      "Epoch [8/10], Step [20/40], Average Loss: 0.0208, Perplexity: 16.8229, Elapsed time: 1962.7515s\n",
      "Best Model saved at 8: ./models/decoder-best.ckpt\n",
      "Best Model saved at 8: ./models/encoder-best.ckpt\n",
      "[ Training ]\n",
      "Epoch [9/10], Step [0/235], Average Loss: 0.0153, Perplexity: 7.0646, Elapsed time: 1981.7024s\n",
      "Epoch [9/10], Step [20/235], Average Loss: 0.0151, Perplexity: 6.7492, Elapsed time: 1999.3359s\n",
      "Epoch [9/10], Step [40/235], Average Loss: 0.0151, Perplexity: 6.9503, Elapsed time: 2016.9769s\n",
      "Epoch [9/10], Step [60/235], Average Loss: 0.0151, Perplexity: 6.8200, Elapsed time: 2034.5871s\n",
      "Epoch [9/10], Step [80/235], Average Loss: 0.0151, Perplexity: 7.1918, Elapsed time: 2052.1760s\n",
      "Epoch [9/10], Step [100/235], Average Loss: 0.0151, Perplexity: 7.4132, Elapsed time: 2069.8098s\n",
      "Epoch [9/10], Step [120/235], Average Loss: 0.0152, Perplexity: 6.7648, Elapsed time: 2087.5380s\n",
      "Epoch [9/10], Step [140/235], Average Loss: 0.0152, Perplexity: 6.9062, Elapsed time: 2105.2998s\n",
      "Epoch [9/10], Step [160/235], Average Loss: 0.0152, Perplexity: 7.6628, Elapsed time: 2123.4121s\n",
      "Epoch [9/10], Step [180/235], Average Loss: 0.0152, Perplexity: 7.6301, Elapsed time: 2141.4283s\n",
      "Epoch [9/10], Step [200/235], Average Loss: 0.0152, Perplexity: 7.3436, Elapsed time: 2159.2302s\n",
      "Epoch [9/10], Step [220/235], Average Loss: 0.0153, Perplexity: 6.4571, Elapsed time: 2177.2158s\n",
      "Model saved: ./models/decoder-9.ckpt\n",
      "Model saved: ./models/encoder-9.ckpt\n",
      "[ Validation ]\n",
      "Epoch [9/10], Step [0/40], Average Loss: 0.0210, Perplexity: 14.7717, Elapsed time: 2190.6674s\n",
      "Epoch [9/10], Step [20/40], Average Loss: 0.0209, Perplexity: 17.2401, Elapsed time: 2210.0312s\n",
      "[ Training ]\n",
      "Epoch [10/10], Step [0/235], Average Loss: 0.0141, Perplexity: 6.0651, Elapsed time: 2228.3402s\n",
      "Epoch [10/10], Step [20/235], Average Loss: 0.0143, Perplexity: 6.3434, Elapsed time: 2246.0489s\n",
      "Epoch [10/10], Step [40/235], Average Loss: 0.0144, Perplexity: 6.6226, Elapsed time: 2263.6082s\n",
      "Epoch [10/10], Step [60/235], Average Loss: 0.0144, Perplexity: 6.2153, Elapsed time: 2281.6362s\n",
      "Epoch [10/10], Step [80/235], Average Loss: 0.0144, Perplexity: 6.4767, Elapsed time: 2299.5551s\n",
      "Epoch [10/10], Step [100/235], Average Loss: 0.0144, Perplexity: 6.6247, Elapsed time: 2317.4056s\n",
      "Epoch [10/10], Step [120/235], Average Loss: 0.0145, Perplexity: 6.4612, Elapsed time: 2335.1070s\n",
      "Epoch [10/10], Step [140/235], Average Loss: 0.0145, Perplexity: 5.8517, Elapsed time: 2352.7774s\n",
      "Epoch [10/10], Step [160/235], Average Loss: 0.0145, Perplexity: 6.5998, Elapsed time: 2370.6445s\n",
      "Epoch [10/10], Step [180/235], Average Loss: 0.0145, Perplexity: 6.5214, Elapsed time: 2388.2275s\n",
      "Epoch [10/10], Step [200/235], Average Loss: 0.0145, Perplexity: 6.6699, Elapsed time: 2405.8234s\n",
      "Epoch [10/10], Step [220/235], Average Loss: 0.0145, Perplexity: 7.1399, Elapsed time: 2423.4655s\n",
      "Model saved: ./models/decoder-10.ckpt\n",
      "Model saved: ./models/encoder-10.ckpt\n",
      "[ Validation ]\n",
      "Epoch [10/10], Step [0/40], Average Loss: 0.0213, Perplexity: 15.2409, Elapsed time: 2436.7673s\n",
      "Epoch [10/10], Step [20/40], Average Loss: 0.0210, Perplexity: 17.1674, Elapsed time: 2456.2631s\n",
      "[!] Best model saved at 8...\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "log_step = 20\n",
    "start_time = time.time()\n",
    "best_valid_perplexity = 9999999999999999\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"[ Training ]\")\n",
    "    total_loss = 0\n",
    "    total_count = 0\n",
    "    total_step = len(train_data_loader)\n",
    "    for i, (images, captions, lengths) in enumerate(train_data_loader):\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "        targets = pack_padded_sequence(captions, lengths, batch_first=True)[0]\n",
    "\n",
    "        features = encoder(images)\n",
    "        outputs = decoder(features, captions, lengths)\n",
    "        loss = criterion(outputs, targets)\n",
    "        decoder.zero_grad()\n",
    "        encoder.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_count += images.shape[0]\n",
    "\n",
    "        if i % log_step == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Average Loss: {:.4f}, Perplexity: {:5.4f}, Elapsed time: {:.4f}s'\n",
    "                  .format(epoch+1, num_epochs, i, total_step, total_loss / total_count, np.exp(loss.item()), time.time() - start_time))\n",
    "\n",
    "    torch.save(decoder.state_dict(), os.path.join(model_path, f'decoder-{epoch + 1}.ckpt'))\n",
    "    torch.save(encoder.state_dict(), os.path.join(model_path, f'encoder-{epoch + 1}.ckpt'))\n",
    "    print(f\"Model saved: {os.path.join(model_path, f'decoder-{epoch + 1}.ckpt')}\")\n",
    "    print(f\"Model saved: {os.path.join(model_path, f'encoder-{epoch + 1}.ckpt')}\")\n",
    "\n",
    "    print(\"[ Validation ]\")\n",
    "    total_loss = 0\n",
    "    total_count = 0\n",
    "    total_step = len(valid_data_loader)\n",
    "    with torch.no_grad():\n",
    "        for i, (images, captions, lengths) in enumerate(valid_data_loader):\n",
    "            images = images.to(device)\n",
    "            captions = captions.to(device)\n",
    "            targets = pack_padded_sequence(captions, lengths, batch_first=True)[0]\n",
    "\n",
    "            features = encoder(images)\n",
    "            outputs = decoder(features, captions, lengths)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_count += images.shape[0]\n",
    "\n",
    "            if i % log_step == 0:\n",
    "                print('Epoch [{}/{}], Step [{}/{}], Average Loss: {:.4f}, Perplexity: {:5.4f}, Elapsed time: {:.4f}s'\n",
    "                      .format(epoch+1, num_epochs, i, total_step, total_loss / total_count, np.exp(loss.item()), time.time() - start_time))\n",
    "\n",
    "    ####### You can mount your drive and save the python, checkpoint and data files to your drive to prevent the elimination of related files by runtime disconnection.\n",
    "\n",
    "    if best_valid_perplexity >= np.exp(total_loss / total_count):\n",
    "        torch.save(decoder.state_dict(), os.path.join(model_path, 'decoder-best.ckpt'))\n",
    "        torch.save(encoder.state_dict(), os.path.join(model_path, 'encoder-best.ckpt'))\n",
    "        print(f\"Best Model saved at {epoch+1}: {os.path.join(model_path, 'decoder-best.ckpt')}\")\n",
    "        print(f\"Best Model saved at {epoch+1}: {os.path.join(model_path, 'encoder-best.ckpt')}\")\n",
    "\n",
    "        best_valid_perplexity = np.exp(total_loss / total_count)\n",
    "        best_epoch = epoch+1\n",
    "\n",
    "print(f\"[!] Best model saved at {best_epoch}...\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
